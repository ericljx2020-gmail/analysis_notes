{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43641ab-5d45-4652-85ce-869078dd7dbc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f82a373f-d439-4561-8ddf-45affb6df3fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as psf\n",
    "import pyspark.sql.types as pst\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta, date\n",
    "import dateutil.relativedelta as relativedelta\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark import StorageLevel\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as pst\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, lit\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a94fa613-77ad-49e0-a188-58fc6dcf666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/expanse/lustre/scratch/jli21/temp_project/spark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"SPARK_HOME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51ab92-efdb-4a79-adc1-46df445a4bf1",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db8a89f-4897-4b80-9c97-e2b5dfe9eaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jli21'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user=%env USER\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baad2cb7-1555-4d2f-b27a-d40290c41706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp-3-19.expanse.sdsc.edu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname = !hostname --fqdn\n",
    "hostname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e20e283e-7bf9-461b-b873-0255dc49a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/opt/anaconda3/envs/spark3/bin/python\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/local/spark-3.1.2-bin-hadoop3.2\"\n",
    "os.environ[\"HADOOP_OPTS\"] = \"-Djava.library.path=/cm/shared/apps/spack/cpu/opt/spack/linux-centos8-zen/gcc-8.3.1/hadoop/3.2.2/lib/native\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = fr\"--jars /expanse/lustre/scratch/{user}/temp_project/spark-3.2.1/ext_jars/* --packages org.apache.spark:spark-avro_2.12:3.2.1 pyspark-shell\"\n",
    "os.environ[\"SPARK_LIB\"] = os.environ[\"SPARK_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb34494-1de7-4c80-ac44-4098a5b68d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x1555311e6970>\n"
     ]
    }
   ],
   "source": [
    "# Set your credentials here\n",
    "UCSD_NT_S3_ACCESS_KEY = \"163a6d8005ad4d1d92faaf71993ef4ae\"\n",
    "UCSD_NT_S3_SECRET_KEY = \"8a1dfbbba6e5485088bad4a9882a44b7\"\n",
    "\n",
    "MASTER_URL=fr\"spark://{hostname[0]}:7077\"\n",
    "\n",
    "spark = SparkSession.builder.master(MASTER_URL).appName('spark-cluster'\n",
    ").config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    ").config(\"fs.s3a.access.key\", UCSD_NT_S3_ACCESS_KEY\n",
    ").config(\"fs.s3a.secret.key\", UCSD_NT_S3_SECRET_KEY\n",
    ").config(\"fs.s3a.endpoint\", \"https://hermes.caida.org\").config(\"fs.s3a.path.style.access\", \"true\"\n",
    ").config(\"fs.s3a.block.size\", \"64M\").config(\"fs.s3a.readahead.range\", \"128K\"\n",
    ").config(\"fs.s3a.experimental.input.fadvise\", \"sequential\"\n",
    ").config(\"fs.s3a.connection.maximum\", 256\n",
    ").config(\"spark.cores.max\", \"128\"\n",
    ").config(\"spark.driver.cores\",\"2\"\n",
    ").config(\"spark.driver.memory\",\"2G\"\n",
    ").config(\"spark.executor.cores\", \"125\"\n",
    ").config(\"spark.executor.memory\", \"125G\"\n",
    "# ).config(\"spark.executor.memoryOverhead\", \"4G\"\n",
    ").config(\"io.file.buffer.size\", \"67108864\"\n",
    ").config(\"spark.submit.deploymode\", \"client\"\n",
    ").config(\"spark.buffer.size\", \"67108864\"\n",
    ").config(\"spark.network.timeout\", \"300s\"\n",
    ").config(\"spark.sql.session.timeZone\", \"UTC\"\n",
    ").config(\"spark.sql.files.ignoreCorruptFiles\", \"true\" # Set this when analyzing periods of high volatility, during which there may be corrupt files\n",
    ").getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c42941a5-efa4-4e54-b754-c5604e9fc370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jli21/miniconda3/envs/tele_ts/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# sc = SparkContext(conf=spark)\n",
    "# sc.setLogLevel('ERROR')\n",
    "sqlcontext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ddc5b72-6f65-4974-a90e-4b71e6930280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://exp-3-19.expanse.sdsc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://exp-3-19.expanse.sdsc.edu:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-cluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1555311e6970>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65076db-466e-48b8-89b4-0f2277462da8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4780b1c-5031-4046-a92b-1e456d7cfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket, struct\n",
    "import ipaddress\n",
    "\n",
    "def long2ip(long):\n",
    "    return ipaddress.ip_network(socket.inet_ntoa(struct.pack('!L', long)) + '/255.255.255.255', strict=False)\n",
    "\n",
    "def long2ip24subnet(long):\n",
    "    return ipaddress.ip_network(socket.inet_ntoa(struct.pack('!L', long)) + '/255.255.255.0', strict=False)\n",
    "\n",
    "def long2ip16subnet(long):\n",
    "    return ipaddress.ip_network(socket.inet_ntoa(struct.pack('!L', long)) + '/255.255.0.0', strict=False)\n",
    "\n",
    "'''\n",
    "Convert an IP string to long\n",
    "'''\n",
    "def ip2long(ip):\n",
    "    packedIP = socket.inet_aton(ip)\n",
    "    return struct.unpack(\"!L\", packedIP)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2477ca8a-2ef9-4c6a-98ff-7f864512269d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Input: List of IPv4 Subnets in CIDR Notation e.g. ('0.0.0.0/16')\n",
    "\n",
    "Returns the set of all IP addresses as integers within the subnet.\n",
    "'''\n",
    "def explode_prefix(prefix_list):\n",
    "    prefix_set = set()\n",
    "    \n",
    "    for pfx in prefix_list:\n",
    "        for ip in ipaddress.IPv4Network(pfx):\n",
    "            prefix_set.add(ip2long(str(ip)))\n",
    "            \n",
    "    return prefix_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ade5dac5-4f22-45e0-86a4-afbeb07aed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "def load_ft_pyspark(avro_files):\n",
    "    df = sqlcontext.read.format('avro').load(avro_files)\n",
    "    df = df.withColumn('time', psf.to_utc_timestamp((df.time).cast(dataType=pst.TimestampType()), 'UTC'))\n",
    "    # df = df.withColumn('time', psf.from_unixtime(df.time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ba379-a9f3-4c74-bae9-b857c320c17f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bb061-db69-481f-aaeb-82291124157f",
   "metadata": {},
   "source": [
    "### Optional Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d1b94-6506-4d84-8b6f-495614ce6d8c",
   "metadata": {},
   "source": [
    "#### Source Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc3e72ca-16d0-48a3-b91e-16096f4e6e4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "VICTIM_24 = '157.42.200.0/24'\n",
    "START = \"2023-06-15 00:00\"\n",
    "END = \"2023-06-17 23:59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80f8a-259c-4af8-9b35-ec7451bf41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "START += \" 00:00\"\n",
    "END += \" 23:59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f46b25f-b2ce-42d9-912c-abd093b1894d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_ip_filter = set([\n",
    "    # '137.110.41.27',\n",
    "    # '137.110.41.248',\n",
    "    # '137.110.33.50',\n",
    "    # '137.110.60.41',\n",
    "])\n",
    "\n",
    "src_subnet_filter = set([\n",
    "    # '137.110.0.0/16'\n",
    "    # '1.116.129.0/24',\n",
    "    VICTIM_24,\n",
    "])\n",
    "\n",
    "src_asn_filter = [\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c69a26dd-e651-4c4c-a608-ac30c21c7f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP filter cardinality: 256\n",
      "ASN filter cardinality: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert to integers\n",
    "src_ip_filter = set(map(ip2long, src_ip_filter))\n",
    "src_subnet_filter = explode_prefix(src_subnet_filter)\n",
    "# Union\n",
    "ip_filter = src_ip_filter.union(src_subnet_filter)\n",
    "\n",
    "print(f'IP filter cardinality: {len(ip_filter)}')\n",
    "print(f'ASN filter cardinality: {len(src_asn_filter)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83a2b5-1be7-41d2-b86d-b753d9352f14",
   "metadata": {},
   "source": [
    "#### Dest. Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a03a7901-4b1d-4e98-8028-8663527a2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ip_filter = set([\n",
    "])\n",
    "\n",
    "dst_port_filter = set([\n",
    "    # 22,\n",
    "    # 1883\n",
    "])\n",
    "\n",
    "dst_port_neg_filter = set([\n",
    "    # 22, \n",
    "    # 23, \n",
    "    # 80, \n",
    "    # 81,\n",
    "    # 445,\n",
    "    # 1433, \n",
    "    # 2323, \n",
    "    # 3389, \n",
    "    # 5555,\n",
    "    # 8080,\n",
    "    # 52869\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801bb8c6-7a4c-4ac6-a56e-e656659b5959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18714bc7-8fa2-4d68-86d5-1cc30991d8e1",
   "metadata": {},
   "source": [
    "#### Temporal Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f13f9edd-01a6-4570-8baa-fd7fff2071b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inclusive start and end times\n",
    "start = pd.to_datetime(START)\n",
    "end = pd.to_datetime(END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106087c-b139-4263-92b5-b82402c4fee3",
   "metadata": {},
   "source": [
    "### Load .avro files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "620615e6-2550-4371-9054-c8cbca2fd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamp(filename):\n",
    "    return int(filename.split('.')[1])\n",
    "\n",
    "def build_uri(row):\n",
    "    return f's3a://{row.container}/{row.datasource}/year={row.year}/month={row.month}/day={row.day}/hour={row.hour}/{row.filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bea00da-e518-42fa-9f8a-608f2f907f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avro filecount: 852\n",
      "Avro uri count: 852\n"
     ]
    }
   ],
   "source": [
    "avro_df = pd.read_parquet('ft4_file_lists/telescope-ucsdnt-avro-flowtuple-v4-2023.parquet.gzip')\n",
    "\n",
    "### Reformatting\n",
    "avro_df['datetime2'] = avro_df['filename'].apply(lambda x: extract_timestamp(x))\n",
    "avro_df['datetime2'] = pd.to_datetime(avro_df['datetime2'], unit='s')\n",
    "avro_df = avro_df.set_index('datetime2')\n",
    "# print(avro_df.head(3))\n",
    "\n",
    "### Select .avro's within timeframe\n",
    "selected_avros = avro_df[(avro_df.index >= start) & (avro_df.index <= end)]\n",
    "print(f'Avro filecount: {len(selected_avros)}')\n",
    "\n",
    "### Build URI's\n",
    "selected_avro_uris = selected_avros.apply(lambda x: build_uri(x), axis=1).values\n",
    "print(f'Avro uri count: {len(selected_avro_uris)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "763a26ee-20ae-4e47-8809-a18c2e5f6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/24 21:14:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 ms, sys: 6.01 ms, total: 44.8 ms\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load .avro's\n",
    "spark_df = load_ft_pyspark(list(selected_avro_uris))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e203cf4-969a-407f-be21-94189f1353d7",
   "metadata": {},
   "source": [
    "### Apply Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03258d13-882f-4a14-8125-79358aa176e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter_df(items, col_name):\n",
    "    return spark.createDataFrame(\n",
    "        [(item,) for item in items],\n",
    "        [col_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d128b42e-59ff-48ca-9648-cb264ccd8bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Source IP Filter\n"
     ]
    }
   ],
   "source": [
    "traff_df = spark_df\n",
    "\n",
    "if len(ip_filter) > 0:\n",
    "    print(f'Applying Source IP Filter')\n",
    "    ip_filter_df = create_filter_df(ip_filter, 'src_ip')\n",
    "    traff_df = traff_df.join(\n",
    "        F.broadcast(\n",
    "            ip_filter_df\n",
    "            ),\n",
    "        on='src_ip'\n",
    "    )\n",
    "\n",
    "if len(src_asn_filter) > 0:\n",
    "    print(f'Applying Source ASN Filter')\n",
    "    asn_filter_df = create_filter_df(src_asn_filter, 'prefix2asn')\n",
    "    traff_df = traff_df.join(\n",
    "        F.broadcast(\n",
    "            asn_filter_df\n",
    "            ),\n",
    "        on='prefix2asn'\n",
    "    )\n",
    "    \n",
    "\n",
    "if len(dst_port_filter) > 0:\n",
    "    print(f'Applying Dest. Port Filter')\n",
    "    dp_filter_df = create_filter_df(dst_port_filter, 'dst_port')\n",
    "    traff_df = traff_df.join(\n",
    "        F.broadcast(\n",
    "            dp_filter_df\n",
    "            ),\n",
    "        on='dst_port'\n",
    "    )\n",
    "\n",
    "if len(dst_port_neg_filter) > 0:\n",
    "    print(f'Applying Dest. Port Negative Filter')\n",
    "    dp_filter_df = create_filter_df(dst_port_neg_filter, 'dst_port')\n",
    "    traff_df = traff_df.join(\n",
    "        F.broadcast(\n",
    "            dp_filter_df\n",
    "            ),\n",
    "        on='dst_port',\n",
    "        how='leftanti'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11b4fd-fd0d-40bb-9e8c-03b98ef2da8f",
   "metadata": {},
   "source": [
    "### Diagnostic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd16cabe-b9f3-41cc-9029-a6e9e093e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def lineplot(series, label, title, ylabel, xlabel):        \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(series, label=f'{label}', marker='.')\n",
    "    plt.legend()\n",
    "\n",
    "    # plt.gca().xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0, 30]))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(byhour=[0, 6, 12, 18]))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d \\n %H:%M\"))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    plt.title(f'{title}', fontsize=14)\n",
    "    plt.ylabel(f'{ylabel}', fontsize=14)\n",
    "    plt.xlabel(f'{xlabel}', fontsize=14)\n",
    "    plt.gca().xaxis.set_tick_params(labelsize=8)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2e76141-5b58-4baf-be0e-e72cb3d3bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_ecdf(df, x, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.ecdfplot(data=df, x=x, log_scale=(True, False))\n",
    "\n",
    "    plt.ylabel(f'{ylabel}', fontsize=14)\n",
    "    plt.xlabel(f'{xlabel}', fontsize=14)\n",
    "    plt.title(f'{title}', fontsize=14)\n",
    "    plt.gca().set_xlim([0.1, 1e6])\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9288cfb2-3d9a-49b4-835d-1fd8fa172a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2 µs, total: 2 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# traff_df = traff_df.cache() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02071663-d8ce-4ddb-a94d-dc51adc6a251",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Group by the 'time' column and calculate the sum of 'packet_cnt'\n",
    "grouped_df = traff_df.groupBy([\"time\", \"protocol\"]).agg(sum(\"packet_cnt\").alias(\"total_packet_count\"))\n",
    "\n",
    "# Select only the 'time' and 'total_packet_count' columns\n",
    "result_df = grouped_df.select(\"time\", \"protocol\", \"total_packet_count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b93120b-6381-4f60-81fa-695e1b1bf6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jli21/miniconda3/envs/tele_ts/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:194: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df = result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ab355f7-b070-489c-b9fc-aa4f308d62e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_parquet(f'./bgp_parquet/[{start}-{VICTIM_24[:-3]}].parquet.gzip', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
